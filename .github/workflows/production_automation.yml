name: Production Automation

on:
  # Scheduled runs
  schedule:
    - cron: '0 0 1 * *'  # Run at midnight on the first day of every month
  # Manual trigger
  workflow_dispatch:
  # On push to main
  push:
    branches:
      - main
    paths:
      - 'src/**'
      - 'scripts/**'
      - 'tests/**'
      - '.github/workflows/production_automation.yml'
  # On PR review approval
  pull_request_review:
    types: [submitted]

jobs:
  test:
    if: |
      github.event_name == 'schedule'
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install poetry
        poetry install
    
    - name: Set up environment
      run: |
        echo "MLFLOW_TRACKING_URI=http://127.0.0.1:5001" >> $GITHUB_ENV
        echo "DATABASE_URL_PYMSSQL=mssql+pymssql://myuser:Password1!@employeeattrition2.database.windows.net/EmployeeAttrition" >> $GITHUB_ENV
        echo "DATABASE_URL_PYODBC=mssql+pyodbc://myuser:Password1!@employeeattrition2.database.windows.net/EmployeeAttrition?driver=ODBC+Driver+17+for+SQL+Server" >> $GITHUB_ENV
    
    - name: Run tests
      run: |
        poetry run pytest tests/
        
    - name: Run linting
      run: |
        poetry run black . --check
        poetry run isort . --check
        poetry run flake8 .
        poetry run mypy .


  run_production_automation:
    # Run on schedule, manual trigger, or push to main
    if: |
      github.event_name == 'schedule' || 
      github.event_name == 'workflow_dispatch' || 
      (github.event_name == 'push' && github.ref == 'refs/heads/main')
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install poetry
        poetry install
    
    - name: Set up environment
      run: |
        echo "MLFLOW_TRACKING_URI=http://127.0.0.1:5001" >> $GITHUB_ENV
        echo "DATABASE_URL_PYMSSQL=mssql+pymssql://myuser:Password1!@employeeattrition2.database.windows.net/EmployeeAttrition" >> $GITHUB_ENV
        echo "DATABASE_URL_PYODBC=mssql+pyodbc://myuser:Password1!@employeeattrition2.database.windows.net/EmployeeAttrition?driver=ODBC+Driver+17+for+SQL+Server" >> $GITHUB_ENV
       
    - name: Start MLflow server
      run: |
        poetry run mlflow server --host 127.0.0.1 --port 5001 &
        sleep 5  # Wait for server to start
        
    # Step 1: Create drift reference
    - name: Create drift reference
      id: create_reference
      run: |
        poetry run python scripts/create_drift_reference.py
    # Step 2: Run drift detection
    - name: Run drift detection
      id: drift
      continue-on-error: true
      run: |
        poetry run python -m employee_attrition_mlops.drift_detection
     
    # Step 3: If drift detected, retrain model
    - name: Retrain model if drift detected
      if: steps.drift.outcome == 'failure' && steps.drift.exitcode == 1
      id: retrain
      run: |
        echo "Drift detected. Starting retraining process..."
        poetry run python scripts/optimize_train_select.py
      
    # Step 4: If no drift, run batch prediction
    - name: Run batch prediction
      if: steps.drift.outcome == 'success'
      id: batch_predict
      run: |
        poetry run python scripts/batch_predict.py
        
    # Step 5: Create summary issue
    - name: Create summary issue
      if: always()
      uses: actions/github-script@v6
      with:
        script: |
          const fs = require('fs');
          let summary = '## Production Automation Summary\n\n';
          
          // Add drift detection results if available
          if (fs.existsSync('reports/drift_report.json')) {
            const driftReport = JSON.parse(fs.readFileSync('reports/drift_report.json', 'utf8'));
            summary += `### Drift Detection Results\n`;
            summary += `- Drift detected: ${driftReport.dataset_drift}\n`;
            summary += `- Drift share: ${(driftReport.drift_share * 100).toFixed(2)}%\n`;
            summary += `- Number of drifted features: ${driftReport.n_drifted_features}\n\n`;
          }
          
          // Add retraining results if available
          if (fs.existsSync('reports/retraining_summary.json')) {
            const retrainSummary = JSON.parse(fs.readFileSync('reports/retraining_summary.json', 'utf8'));
            summary += `### Retraining Results\n`;
            summary += `- Was retrained: ${retrainSummary.was_retrained}\n`;
            summary += `- New model performance: ${retrainSummary.new_model_performance}\n`;
            summary += `- Performance improvement: ${retrainSummary.performance_improvement}\n\n`;
          }
          
          // Add optimization results if available
          if (fs.existsSync('reports/optimization_summary.json')) {
            const optSummary = JSON.parse(fs.readFileSync('reports/optimization_summary.json', 'utf8'));
            summary += `### Model Optimization Results\n`;
            summary += `- Selected model type: ${optSummary.selected_model_type}\n`;
            summary += `- Best hyperparameters: ${JSON.stringify(optSummary.best_hyperparameters)}\n`;
            summary += `- Cross-validation score: ${optSummary.cv_score}\n\n`;
          }
          
          // Add batch prediction results if available
          if (fs.existsSync('reports/batch_prediction_summary.json')) {
            const predSummary = JSON.parse(fs.readFileSync('reports/batch_prediction_summary.json', 'utf8'));
            summary += `### Batch Prediction Results\n`;
            summary += `- Number of predictions: ${predSummary.num_predictions}\n`;
            summary += `- Attrition rate: ${(predSummary.attrition_rate * 100).toFixed(2)}%\n\n`;
          }
          
          // Add metrics results if available
          if (fs.existsSync('reports/metrics_summary.json')) {
            const metricsSummary = JSON.parse(fs.readFileSync('reports/metrics_summary.json', 'utf8'));
            summary += `### Model Metrics\n`;
            summary += `- Accuracy: ${(metricsSummary.accuracy * 100).toFixed(2)}%\n`;
            summary += `- Precision: ${(metricsSummary.precision * 100).toFixed(2)}%\n`;
            summary += `- Recall: ${(metricsSummary.recall * 100).toFixed(2)}%\n`;
            summary += `- F1 Score: ${(metricsSummary.f1_score * 100).toFixed(2)}%\n\n`;
          }
          
          // Add fairness results if available
          if (fs.existsSync('reports/fairness_summary.json')) {
            const fairnessSummary = JSON.parse(fs.readFileSync('reports/fairness_summary.json', 'utf8'));
            summary += `### Fairness Metrics\n`;
            for (const [feature, metrics] of Object.entries(fairnessSummary)) {
              summary += `#### ${feature}\n`;
              summary += `- Demographic Parity Difference: ${metrics.demographic_parity_difference.toFixed(4)}\n`;
              summary += `- Equalized Odds Difference: ${metrics.equalized_odds_difference.toFixed(4)}\n\n`;
            }
          }
          
          // Add error information if available
          if (fs.existsSync('reports/automation_error.json')) {
            const errorSummary = JSON.parse(fs.readFileSync('reports/automation_error.json', 'utf8'));
            summary += `### Error Information\n`;
            summary += `- Error type: ${errorSummary.error_type}\n`;
            summary += `- Error message: ${errorSummary.error}\n\n`;
          }
          
          // Add MLflow run information
          summary += `### MLflow Run Information\n`;
          summary += `- Run ID: ${{ steps.retrain.outputs.run_id || steps.batch_predict.outputs.run_id }}\n`;
          summary += `- MLflow UI: ${{ secrets.MLFLOW_TRACKING_URI }}/#/experiments/0/runs/${{ steps.retrain.outputs.run_id || steps.batch_predict.outputs.run_id }}\n\n`;
          
          // Create issue with summary
          await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: `Production Automation ${context.job.status === 'success' ? '✅' : '❌'} - ${new Date().toISOString().split('T')[0]}`,
            body: summary,
            labels: context.job.status === 'success' ? ['automation-success'] : ['automation-failed']
          }); 